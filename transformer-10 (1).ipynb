{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Grupo Transformer 10**\nEn este notebook vamos a construir dos distintos tipos de redes neuronales con la finalidad de que estos puedan clasificar especies de aves a través de imagenes. Como primer tipo harémos una red neuronal sencilla y por otro lado una con la arquitectura Transformer, de mayor complejidad, la cual deberá realizar mejor la tarea.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:27.225856Z","iopub.execute_input":"2022-06-24T10:50:27.226480Z","iopub.status.idle":"2022-06-24T10:50:40.914243Z","shell.execute_reply.started":"2022-06-24T10:50:27.226349Z","shell.execute_reply":"2022-06-24T10:50:40.913072Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from typing import List, Union\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torchvision import datasets\nfrom torch import nn\nfrom datetime import datetime\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nimport random\nimport copy","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:40.916644Z","iopub.execute_input":"2022-06-24T10:50:40.917097Z","iopub.status.idle":"2022-06-24T10:50:43.309290Z","shell.execute_reply.started":"2022-06-24T10:50:40.917050Z","shell.execute_reply":"2022-06-24T10:50:43.308331Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Lectura de Datasets\nHemos utilizado esta clase (*BirdsDataset*) para leer el dataset de submission test, ya que con ella guardamos el nombre de la imagen que se va a leer y con ello podremos obtener la id de la imagen para escribir el csv por el que se nos evalúa, el cual debe de quedar de esta forma: Id,Category","metadata":{}},{"cell_type":"code","source":"\n\nclass BirdsDataset(torch.utils.data.Dataset):\n    def __init__(self, path: Union[Path, str], transform: Union['Transform', List['Transform']] = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor(), transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])])):\n        self.path = Path(path)\n        self.labels = [p.name for p in path.glob('*')]\n        self.images = list(path.glob('*/*.jpg'))\n        self.transform = transform\n        \n    def __len__(self) -> int:\n        return len(self.images)\n    \n    def __getitem__(self, index:int) -> torch.Tensor:\n        image_path = self.images[index]\n        label = image_path.name\n        #Las imagenes deben ser PIL para que pueda crearse como Tensor\n        image = self.transform(Image.open((str(image_path))))\n        return image, label\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-24T10:50:43.311568Z","iopub.execute_input":"2022-06-24T10:50:43.312529Z","iopub.status.idle":"2022-06-24T10:50:43.324021Z","shell.execute_reply.started":"2022-06-24T10:50:43.312485Z","shell.execute_reply":"2022-06-24T10:50:43.322771Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Para guardar el dataset de entrenamiento, hacemos uso de la función que nos proporciona pyTorch de ImageFolder, el cual se encargará de guardar las imagenes, con la etiqueta de la especie en forma de número según la carpeta en la que se encuentre cada imagen. Para pasar de esta etiqueta en forma de número a el nombre de la especie, haremos uso del atributo *classes* de este dataset.","metadata":{}},{"cell_type":"code","source":"dataset = datasets.ImageFolder('/kaggle/input/iais22-birds/birds/birds', transform= transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor(),transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])]) )\ntest_data = BirdsDataset(\n    path= Path('/kaggle/input/iais22-birds/submission_test'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:43.328470Z","iopub.execute_input":"2022-06-24T10:50:43.328810Z","iopub.status.idle":"2022-06-24T10:50:50.491951Z","shell.execute_reply.started":"2022-06-24T10:50:43.328768Z","shell.execute_reply":"2022-06-24T10:50:50.490775Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"También haremos uso de la clase DataLoader, la cual nos facilita introducir datos a la red neuronal posteriormente.","metadata":{}},{"cell_type":"code","source":"batch_size = 64\n\ntrain_dataloader = DataLoader(dataset,shuffle=True, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:50.493619Z","iopub.execute_input":"2022-06-24T10:50:50.494010Z","iopub.status.idle":"2022-06-24T10:50:50.500950Z","shell.execute_reply.started":"2022-06-24T10:50:50.493966Z","shell.execute_reply":"2022-06-24T10:50:50.499730Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Imprimimos la forma de estos dataloader para comprobar que es correcto.\nfor X, y in train_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n\n    break\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n\n    break\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:50.502751Z","iopub.execute_input":"2022-06-24T10:50:50.503938Z","iopub.status.idle":"2022-06-24T10:50:51.294648Z","shell.execute_reply.started":"2022-06-24T10:50:50.503894Z","shell.execute_reply":"2022-06-24T10:50:51.293295Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Shape of X [N, C, H, W]: torch.Size([64, 3, 64, 64]) torch.float32\nShape of y: torch.Size([64]) torch.int64\nShape of X [N, C, H, W]: torch.Size([64, 3, 64, 64]) torch.float32\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creación de la red neuronal sencilla\nHemos definido una red neuronal que tendrá como entrada 3\\*64\\*64, los cuales se refieren a los 3 canales (RGB) y 64 de altura y 64 de anchura que tiene la imagen que le introduciremos. Como salida, deberá tener 400 outputs, debido a las 400 clases por las que debe clasificar. El mayor de estos outputs será la clase predicha por la red.","metadata":{}},{"cell_type":"code","source":"# Utilizamos a ser posible la GPU.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Definición del modelo\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(3*64*64, 12288), # (3 canales, 64 de altura y 64 de anchura)\n            nn.ReLU(),\n            nn.Linear(12288, 6144),\n            nn.ReLU(),\n            nn.Linear(6144, 6144),\n            nn.ReLU(),\n            nn.Linear(6144, 6144),\n            nn.ReLU(),\n            nn.Linear(6144, 400) # 400 es el número de clases\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n        # keep track of training loss\n        epoch_loss = 0.0\n        epoch_accuracy = 0.0\n\n        ###################\n        # train the model #\n        ###################\n        self.train()\n        for i, (data, target) in enumerate(train_loader):\n            # move tensors to GPU if CUDA is available\n            if device.type == \"cuda\":\n                data, target = data.cuda(), target.cuda()\n            \n            # Retropropagación\n            optimizer.zero_grad()\n            output = self.forward(data)\n            loss = criterion(output, target)\n            loss.backward()\n            #Calcula la precisión media por cada batch y la suma al total\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n            epoch_loss += loss\n            epoch_accuracy += accuracy\n            optimizer.step()\n            if i%100==0:\n                print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n\n        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n\nalternativeModel = NeuralNetwork().to(device)\nprint(alternativeModel)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:51.296735Z","iopub.execute_input":"2022-06-24T10:50:51.297903Z","iopub.status.idle":"2022-06-24T10:50:58.329858Z","shell.execute_reply.started":"2022-06-24T10:50:51.297855Z","shell.execute_reply":"2022-06-24T10:50:58.328848Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Using cuda device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=12288, out_features=12288, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=12288, out_features=6144, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=6144, out_features=6144, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=6144, out_features=6144, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=6144, out_features=400, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nseed_everything(1001)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:58.331255Z","iopub.execute_input":"2022-06-24T10:50:58.332512Z","iopub.status.idle":"2022-06-24T10:50:58.340965Z","shell.execute_reply.started":"2022-06-24T10:50:58.332454Z","shell.execute_reply":"2022-06-24T10:50:58.339804Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Entrenamiento\nAquí definimos las funciones necesarias para entrenar el modelo, en el que definiremos que función de optimización utilizaremos y que función de pérdida. En nuestro caso, vamos a utilizar la función Adam como optimización al igual que se utiliza en el paper *Attention is all you need* y *cross entropy loss* como función de perdida.","metadata":{}},{"cell_type":"code","source":"def fit(model, epochs, device, criterion, optimizer, train_loader):\n\n    for epoch in range(1, epochs + 1):\n\n        train_loss, train_acc = model.train_one_epoch(\n            train_loader, criterion, optimizer, device\n        )\n\n        print(f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\")\n\n\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:58.342565Z","iopub.execute_input":"2022-06-24T10:50:58.343024Z","iopub.status.idle":"2022-06-24T10:50:58.359914Z","shell.execute_reply.started":"2022-06-24T10:50:58.342980Z","shell.execute_reply":"2022-06-24T10:50:58.358822Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def runAlternativeModel(epochs):\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    alternativeModel.to(device)\n    \n    #Función de pérdida\n    criterion = nn.CrossEntropyLoss()\n    # Función de optimización\n    optimizer = torch.optim.Adam(alternativeModel.parameters(), lr=1.0e-5)\n    print(\"Iniciando entrenamiento\")\n\n    start_time = datetime.now()\n    print(f\"Start Time: {start_time}\")\n\n\n    logs = fit(\n        model=alternativeModel,\n        epochs=epochs,\n        device=device,\n        criterion=criterion,\n        optimizer=optimizer,\n        train_loader=train_dataloader\n    )\n    print(f\"Execution time: {datetime.now() - start_time}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:58.364109Z","iopub.execute_input":"2022-06-24T10:50:58.365731Z","iopub.status.idle":"2022-06-24T10:50:58.374564Z","shell.execute_reply.started":"2022-06-24T10:50:58.365635Z","shell.execute_reply":"2022-06-24T10:50:58.373265Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"epochs = 6\nrunAlternativeModel(epochs)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:50:58.376045Z","iopub.execute_input":"2022-06-24T10:50:58.377029Z","iopub.status.idle":"2022-06-24T10:51:30.474661Z","shell.execute_reply.started":"2022-06-24T10:50:58.376985Z","shell.execute_reply":"2022-06-24T10:51:30.472960Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Iniciando entrenamiento\nStart Time: 2022-06-24 10:50:58.383262\n\tBATCH 1/913 - LOSS: 5.991030693054199\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/818371306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrunAlternativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_33/3990311035.py\u001b[0m in \u001b[0;36mrunAlternativeModel\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution time: {datetime.now() - start_time}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/1734635842.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, epochs, device, criterion, optimizer, train_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         train_loss, train_acc = model.train_one_epoch(\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/1410665232.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m# move tensors to GPU if CUDA is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3075\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3077\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    print(\"imagenes para testear: \"+str(size))\n    classes=train_dataloader.dataset.classes\n    \n    model.eval()\n    predicted = []\n    ids=[]\n    with torch.no_grad():\n        for X, y in dataloader:\n            X= X.to(device)\n            pred = model(X)\n            decoder(classes, pred, predicted, y, ids)\n            \n    f= open(\"./submission.csv\",\"w\")\n    f.close()\n    f= open(\"./submission.csv\",\"a\")\n    f.write(\"Id,Category\\n\")\n\n    for i in range(predicted.__len__()):\n        f.write(ids[i]+\",\"+predicted[i]+\"\\n\")\n        \n    f.close()\n           \n    \ndef decoder(classes, listPredicted, predicted, y,ids):\n    #Input: \n    #listPredicted=Lista de predichos de 64 imagenes\n    #Esta función añade a las listas de ids y predicted lo obtenido en la predicción\n    #para posteriormente escribir el csv\n    i=0\n    for predict in listPredicted:\n        predicted.append(classes[predict.argmax(0).item()])\n        ids.append(y[i][:-4]) #Quitamos la parte de \".jpg\"\n        i+=1\n        \n  \n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:34.720495Z","iopub.execute_input":"2022-06-24T10:51:34.720868Z","iopub.status.idle":"2022-06-24T10:51:34.731945Z","shell.execute_reply.started":"2022-06-24T10:51:34.720838Z","shell.execute_reply":"2022-06-24T10:51:34.730639Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(\"Testing:\")\n#test(test_dataloader, alternativeModel, nn.CrossEntropyLoss())\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:34.917538Z","iopub.execute_input":"2022-06-24T10:51:34.918660Z","iopub.status.idle":"2022-06-24T10:51:34.924333Z","shell.execute_reply.started":"2022-06-24T10:51:34.918604Z","shell.execute_reply":"2022-06-24T10:51:34.923034Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Testing:\nDone!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transformer\nParametros:","metadata":{}},{"cell_type":"code","source":"# model specific global variables\nLR = 2e-05\nN_EPOCHS = 11\nIMG_SIZE = 224\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:35.262799Z","iopub.execute_input":"2022-06-24T10:51:35.263249Z","iopub.status.idle":"2022-06-24T10:51:35.269289Z","shell.execute_reply.started":"2022-06-24T10:51:35.263218Z","shell.execute_reply":"2022-06-24T10:51:35.268104Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Creación de la red neuronal Transformer\nEl vision Transformer está definido por las siguientes clases:\n* ViT: Modelo completo del vision transformer.\n* Encoder: Compuesto por el modulo attention y feed forward. Se encargará de procesar estos módulos paralelamente el número que venga indicado en la profundidad (*depth*) y tras ello sumar los valores de salida.\n* PreNorm: Normaliza el input antes de ser introducido en el módulo de self attention y feed forward.\n* Attention: Modulo que permite al modelo atender a la información de distintos subespacios.\n* Feed forward: Red neuronal que procesará tanto los datos introducidos al encoder como la salida del Attention.","metadata":{}},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super(ViT, self).__init__()\n        #Tamaño de imagen (224x224)\n        image_height, image_width = (image_size, image_size)\n        #Tamaño de patch (16x16)\n        patch_height, patch_width = (patch_size, patch_size)\n        \n        # Número de imagenes 16x16\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width # (3*16*16)\n        \n        # Batch inicial= 64 x 3 x 224 x 224 (b c h w)\n        # Salida = 64 x 224/16 * 224/16 x 16*16*3 (64 imagenes x Número de patches x patch)\n        self.to_patch_embedding = nn.Sequential( \n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.Linear(patch_dim, dim),\n        )\n        \n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.encoder = Encoder(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape # 64 x 196 x 1024 (64 imagenes, 196 Número de patches)\n        cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b) # 64, 1, 1024\n\n        #Concatena cls token y x\n        x = torch.cat((cls_tokens, x), dim=1) # 64, 196+1, 1024\n\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.encoder(x)\n\n        x = x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n        # keep track of training loss\n        epoch_loss = 0.0\n        epoch_accuracy = 0.0\n\n        ###################\n        # train the model #\n        ###################\n        self.train()\n        for i, (data, target) in enumerate(train_loader):\n            # move tensors to GPU if CUDA is available\n            if device.type == \"cuda\":\n                data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = self.forward(data)\n            loss = criterion(output, target)\n            loss.backward()\n            #Calcula la precisión media por cada batch y la suma al total\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n            epoch_loss += loss\n            epoch_accuracy += accuracy\n            optimizer.step()\n            if i%100==0:\n                print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n\n        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n    \nclass Encoder(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n    \nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n    \nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5 # 1/sqrt(dk)\n\n        self.sm = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.sm(dots) # softmax(Q*K/sqrt(dk))\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v) # softmax(Q*K/sqrt(dk))*V\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:35.606455Z","iopub.execute_input":"2022-06-24T10:51:35.606851Z","iopub.status.idle":"2022-06-24T10:51:35.641622Z","shell.execute_reply.started":"2022-06-24T10:51:35.606822Z","shell.execute_reply":"2022-06-24T10:51:35.640344Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = ViT(\n        image_size = IMG_SIZE,\n        patch_size = 16,\n        num_classes = 400,\n        dim = 1024,\n        depth = 6,\n        heads = 16,\n        mlp_dim = 4096,\n        dropout = 0.01,\n        emb_dropout = 0.01\n    )\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:52:43.574514Z","iopub.execute_input":"2022-06-24T10:52:43.574936Z","iopub.status.idle":"2022-06-24T10:52:44.260024Z","shell.execute_reply.started":"2022-06-24T10:52:43.574906Z","shell.execute_reply":"2022-06-24T10:52:44.258960Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Entrenamiento","metadata":{}},{"cell_type":"code","source":"# create image augmentations\ntransforms_train = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        #transforms.RandomHorizontalFlip(p=0.3), # Para comparar los modelos en igualdad de condiciones,\n        #transforms.RandomVerticalFlip(p=0.3),   # se ha comentado estas transformaciones.\n        #transforms.RandomResizedCrop(IMG_SIZE), \n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)\n\ntransforms_valid = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:30.494786Z","iopub.status.idle":"2022-06-24T10:51:30.495690Z","shell.execute_reply.started":"2022-06-24T10:51:30.495372Z","shell.execute_reply":"2022-06-24T10:51:30.495418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _run():\n    train_dataset = datasets.ImageFolder('/kaggle/input/iais22-birds/birds/birds', transform= transforms_train)\n    train_dataloader = DataLoader(train_dataset,shuffle=True, batch_size=batch_size)\n    \n\n    criterion = nn.CrossEntropyLoss()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    print(\"Iniciando entrenamiento\")\n\n    start_time = datetime.now()\n    print(f\"Start Time: {start_time}\")\n\n\n    logs = fit(\n        model=model,\n        epochs=N_EPOCHS,\n        device=device,\n        criterion=criterion,\n        optimizer=optimizer,\n        train_loader=train_dataloader\n    )\n    print(f\"Execution time: {datetime.now() - start_time}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:30.497492Z","iopub.status.idle":"2022-06-24T10:51:30.498595Z","shell.execute_reply.started":"2022-06-24T10:51:30.498133Z","shell.execute_reply":"2022-06-24T10:51:30.498184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_run()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:30.500258Z","iopub.status.idle":"2022-06-24T10:51:30.501111Z","shell.execute_reply.started":"2022-06-24T10:51:30.500822Z","shell.execute_reply":"2022-06-24T10:51:30.500854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"print(\"Testing:\")\ncriterion = nn.CrossEntropyLoss()\ntest_data = BirdsDataset(\n    path= Path('/kaggle/input/iais22-birds/submission_test'),transform=transforms_valid)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\ntest(test_dataloader, model, criterion)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:30.502801Z","iopub.status.idle":"2022-06-24T10:51:30.503779Z","shell.execute_reply.started":"2022-06-24T10:51:30.503444Z","shell.execute_reply":"2022-06-24T10:51:30.503475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente guardamos el modelo.","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(),\"./modelo_seleccionado.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T10:51:30.505511Z","iopub.status.idle":"2022-06-24T10:51:30.506403Z","shell.execute_reply.started":"2022-06-24T10:51:30.506048Z","shell.execute_reply":"2022-06-24T10:51:30.506077Z"},"trusted":true},"execution_count":null,"outputs":[]}]}